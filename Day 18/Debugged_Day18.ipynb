{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Debugged_Day18.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNxqtPIy0ZPsIN4+1D46Fbn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Likhita-T/FutureMakers/blob/main/Debugged_Day18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Yn8YsBA8pH"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import re\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import regularizers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heFQXhK7BoEi",
        "outputId": "12b9fe2e-7993-4620-8c6a-c694b0ea2bdb"
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39C1GdKvA9oz"
      },
      "source": [
        "NB_WORDS = 10000  \n",
        "NB_START_EPOCHS = 20  \n",
        "BATCH_SIZE = 512   \n",
        "MAX_LEN = 20  \n",
        "df = pd.read_csv(\n",
        "     \"/content/gdrive/MyDrive/Tweets.csv\",\n",
        "    )\n",
        "#root = Path(r'C:\\Users\\Patri\\Desktop\\MLProjects_AI\\MIT_deeplearning\\Tweets.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSitVfyTCAfX"
      },
      "source": [
        "def deep_model(model, X_train, y_train, X_valid, y_valid):\n",
        "    model.compile(optimizer='rmsprop'\n",
        "                  , loss='categorical_crossentropy'\n",
        "                  , metrics=['accuracy'])\n",
        "    \n",
        "    history = model.fit(X_train\n",
        "                       , y_train\n",
        "                       , epochs=NB_START_EPOCHS\n",
        "                       , batch_size=BATCH_SIZE\n",
        "                       , validation_data=(X_valid, y_valid)\n",
        "                       , verbose=0)\n",
        "    return history"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM1xkN1TCD7z"
      },
      "source": [
        "def eval_metric(model, history, metric_name):\n",
        "    metric = history.history[metric_name]\n",
        "    val_metric = history.history['val_' + metric_name]\n",
        "    e = range(1, NB_START_EPOCHS + 1)\n",
        "    plt.plot(e, metric, 'bo', label='Train ' + metric_name)\n",
        "    plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)\n",
        "    plt.xlabel('Epoch number')\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.title('Comparing training and validation ' + metric_name + ' for ' + model.name)\n",
        "    plt.legend()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qkm1LJxBCGAy"
      },
      "source": [
        "def test_model(model, X_train, y_train, X_test, y_test, epoch_stop):\n",
        "    model.fit(X_train\n",
        "              , y_train\n",
        "              , epochs=epoch_stop\n",
        "              , batch_size=BATCH_SIZE\n",
        "              , verbose=0)\n",
        "    results = model.evaluate(X_test, y_test)\n",
        "    print()\n",
        "    print('Test accuracy: {0:.2f}%'.format(results[1]*100))\n",
        "    return results"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XpsrYApCHqt"
      },
      "source": [
        "def remove_stopwords(input_text):\n",
        "    stopwords_list = stopwords.words('english')\n",
        "    whitelist = [\"n't\", \"not\", \"no\"]\n",
        "    words = input_text.split() \n",
        "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
        "    return \" \".join(clean_words) \n",
        "def remove_mentions(input_text):\n",
        "    return re.sub(r'@\\w+', '', input_text)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgOfOd2RCL79"
      },
      "source": [
        "def compare_models_by_metric(model_1, model_2, model_hist_1, model_hist_2, metric):\n",
        "    metric_model_1 = model_hist_1.history[metric]\n",
        "    metric_model_2 = model_hist_2.history[metric]\n",
        "    e = range(1, NB_START_EPOCHS + 1)\n",
        "    \n",
        "    metrics_dict = {\n",
        "        'acc' : 'Training Accuracy',\n",
        "        'loss' : 'Training Loss',\n",
        "        'val_acc' : 'Validation accuracy',\n",
        "        'val_loss' : 'Validation loss'\n",
        "    }\n",
        "    \n",
        "    metric_label = metrics_dict[metric]\n",
        "    plt.plot(e, metric_model_1, 'bo', label=model_1.name)\n",
        "    plt.plot(e, metric_model_2, 'b', label=model_2.name)\n",
        "    plt.xlabel('Epoch number')\n",
        "    plt.ylabel(metric_label)\n",
        "    plt.title('Comparing ' + metric_label + ' between models')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "def optimal_epoch(model_hist):\n",
        "    min_epoch = np.argmin(model_hist.history['val_loss']) + 1\n",
        "    print(\"Minimum validation loss reached in epoch {}\".format(min_epoch))\n",
        "    return min_epoch"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2uvcH60COq2",
        "outputId": "a5ad9f7d-865f-4383-cc04-79d30b8ae8b9"
      },
      "source": [
        "# Data Prepreperation\n",
        "#df = pd.read_csv('Tweets.csv')\n",
        "import nltk\n",
        "#nltk.download()\n",
        "\n",
        "df = df.reindex(np.random.permutation(df.index))  \n",
        "df = df[['text', 'airline_sentiment']]\n",
        "nltk.download(\"stopwords\")\n",
        "df.text = df.text.apply(remove_stopwords).apply(remove_mentions)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viKe6PpQC3tK"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df.text, df.airline_sentiment, test_size=0.1, random_state=37)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5-CMDU3DbAn"
      },
      "source": [
        "tk = Tokenizer(num_words=NB_WORDS,\n",
        "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{\"}~\\t\\n',\n",
        "               lower=True,\n",
        "               char_level=False,\n",
        "               split=' ')\n",
        "tk.fit_on_texts(X_train)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxERayImDc-Q"
      },
      "source": [
        "X_train_oh = tk.texts_to_matrix(X_train, mode='binary')\n",
        "X_test_oh = tk.texts_to_matrix(X_test, mode='binary')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbEOPiYLDgj7"
      },
      "source": [
        "le = LabelEncoder()\n",
        "y_train_le = le.fit_transform(y_train)\n",
        "y_test_le = le.transform(y_test)\n",
        "y_train_oh = to_categorical(y_train_le)\n",
        "y_test_oh = to_categorical(y_test_le)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ub9lXz5De0G"
      },
      "source": [
        "X_train_rest, X_valid, y_train_rest, y_valid = train_test_split(X_train_oh, y_train_oh, test_size=0.1, random_state=37)"
      ],
      "execution_count": 21,
      "outputs": []
    }
  ]
}
